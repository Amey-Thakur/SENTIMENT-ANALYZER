{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SENTIMENT-ANALYSIS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amey-Thakur/SENTIMENTAL-ANALYZER/blob/main/SENTIMENT_ANALYSIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hLdF1oit06A"
      },
      "source": [
        "# Sentiment analysis of IMDB reviews\n",
        "We will start by importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzJqzHv-LceW"
      },
      "source": [
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54C8Oca0LlVR"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npBPD8k_PwRJ"
      },
      "source": [
        "# Importing the data files\n",
        "After importing the necessary libraries now we will read the data files we have two data files here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOvZ5uMyOtTF"
      },
      "source": [
        "imdb_reviews=pd.read_csv(\"/content/imdb_reviews.csv\")\n",
        "test_reviews=pd.read_csv(\"/content/test_reviews.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4a_Zs-8WhcH"
      },
      "source": [
        "first data file contains the imdb reviews and their corresponding sentiments which can be either positive or negative, we are going to use this file as our training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI4qnZOTPFqx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "75094e0e-840b-471b-b3ba-a927076a88a4"
      },
      "source": [
        "imdb_reviews.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;START this film was just brilliant casting lo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;START big hair big boobs bad music and a gian...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;START this has to be one of the worst films o...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;START the &lt;UNK&gt; &lt;UNK&gt; at storytelling the tra...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;START worst mistake of my life br br i picked...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Reviews Sentiment\n",
              "0  <START this film was just brilliant casting lo...  positive\n",
              "1  <START big hair big boobs bad music and a gian...  negative\n",
              "2  <START this has to be one of the worst films o...  negative\n",
              "3  <START the <UNK> <UNK> at storytelling the tra...  positive\n",
              "4  <START worst mistake of my life br br i picked...  negative"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rNGROl5XYrL"
      },
      "source": [
        "the second file is also similar to the first file but we are going to use it as the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGqg6S8OXVmV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "69fcda5d-2b89-43b9-fd24-fa1ba0eeb797"
      },
      "source": [
        "test_reviews.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;START please give this one a miss br br &lt;UNK&gt;...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;START this film requires a lot of patience be...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;START many animation buffs consider &lt;UNK&gt; &lt;UN...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;START i generally love this type of movie how...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;START like some other people wrote i'm a die ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Reviews Sentiment\n",
              "0  <START please give this one a miss br br <UNK>...  negative\n",
              "1  <START this film requires a lot of patience be...  positive\n",
              "2  <START many animation buffs consider <UNK> <UN...  positive\n",
              "3  <START i generally love this type of movie how...  negative\n",
              "4  <START like some other people wrote i'm a die ...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOfnoRhGXogV"
      },
      "source": [
        "# Preprocessing the data\n",
        "We can not pass the string data to our model directly, so we need to transform the string data into integer format.For this we can map each distinct word as a distinct integer for eg.{'this':14 , 'the':1}.We already have a file that contains the mapping from words to integers so we are going to load that file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf5XTq_hAxL6"
      },
      "source": [
        "word_index=pd.read_csv(\"/content/word_indexes.csv\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE47q3yibEIM"
      },
      "source": [
        "The word index file contains mapping from words to integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbkABN8jA_Ye",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "8693286d-33e8-4653-97fd-869671f43109"
      },
      "source": [
        "word_index.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Words</th>\n",
              "      <th>Indexes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tsukino</td>\n",
              "      <td>52009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nunnery</td>\n",
              "      <td>52010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sonja</td>\n",
              "      <td>16819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vani</td>\n",
              "      <td>63954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>woods</td>\n",
              "      <td>1411</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Words  Indexes\n",
              "0  tsukino    52009\n",
              "1  nunnery    52010\n",
              "2    sonja    16819\n",
              "3     vani    63954\n",
              "4    woods     1411"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZYF6pDzbNCe"
      },
      "source": [
        "Next we are going to convert the word_index dataframe into a python dictionary so that we can use it for converting our reviews from string to integer format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuiMcF5XD65R"
      },
      "source": [
        "word_index=dict(zip(word_index.Words,word_index.Indexes))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsScw2ilFqap"
      },
      "source": [
        "word_index[\"<PAD>\"]=0\n",
        "word_index[\"<START\"]=1\n",
        "word_index[\"<UNK>\"]=2\n",
        "word_index[\"<UNUSED>\"]=3"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVEL7jQ6bjbV"
      },
      "source": [
        "Now we define a function review_encoder that encodes the reviews into integer format according to the mapping specified by word_index file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nay4o8dSAqbu"
      },
      "source": [
        "def review_encoder(text):\n",
        "  arr=[word_index[word] for word in text]\n",
        "  return arr"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3mS11cVcGhU"
      },
      "source": [
        "We split the reviews from their corresponding sentiments so that we can preprocess the reviews and sentiments separately and then later pass it to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUhde9eLE5H3"
      },
      "source": [
        "train_data,train_labels=imdb_reviews['Reviews'],imdb_reviews['Sentiment']\n",
        "test_data, test_labels=test_reviews['Reviews'],test_reviews['Sentiment']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBWDVwCNdBJP"
      },
      "source": [
        "Before transforming the reviews as integers we need to tokenize or split the review on the basis of whitespaces\n",
        "For eg.the string \"The movie was wonderful\" becomes [\"The\" , \"movie\" , \"was\" , \"wonderful\" ]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frWCqvycL-w5"
      },
      "source": [
        "train_data=train_data.apply(lambda review:review.split())\n",
        "test_data=test_data.apply(lambda review:review.split())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GCM_74ve7OZ"
      },
      "source": [
        "Since we have tokenized the reviews now we can apply the review_encoder function to each review and transform the reviews into integer format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJLwZH1OMPJM"
      },
      "source": [
        "train_data=train_data.apply(review_encoder)\n",
        "test_data=test_data.apply(review_encoder)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8kdTaoOfj8d"
      },
      "source": [
        "After transforming, our reviews are going to look like this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3M_klO4MhFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725108ca-534a-4635-c893-d90b9922b302"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, ...\n",
              "1    [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463,...\n",
              "2    [1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5...\n",
              "3    [1, 4, 2, 2, 33, 2804, 4, 2040, 432, 111, 153,...\n",
              "4    [1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 1...\n",
              "Name: Reviews, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEUf78Aif02A"
      },
      "source": [
        "We also need to encode the sentiments and we are labeling the positive sentiment as 1 and negative sentiment as 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p01DiByEJ7fw"
      },
      "source": [
        "def encode_sentiments(x):\n",
        "  if x=='positive':\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "train_labels=train_labels.apply(encode_sentiments)\n",
        "test_labels=test_labels.apply(encode_sentiments)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPOP1mPd1mrh"
      },
      "source": [
        "Before giving the review as an input to the model we need to perform following preprocessing steps:\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "*   The length of each review should be made equal for the model to be working correctly.\n",
        "\n",
        "*  We have chosen the length of each review to be 500. \n",
        "*     If the review is longer than 500 words we are going to cut the extra part of the review.\n",
        "\n",
        "\n",
        "*       If the review is contains less than 500 words we are going to pad the review with zeros to increase its length to 500.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHEVR3TbPE_1"
      },
      "source": [
        "train_data=keras.preprocessing.sequence.pad_sequences(train_data,value=word_index[\"<PAD>\"],padding='post',maxlen=500)\n",
        "test_data=keras.preprocessing.sequence.pad_sequences(test_data,value=word_index[\"<PAD>\"],padding='post',maxlen=500)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjL1Q8kG2YXV"
      },
      "source": [
        "#Building the model\n",
        "Our model is a neural network and it consits of the following layers : \n",
        "\n",
        "1.   one word embedding layer which creates word embeddings of length 16 from integer encoded review.\n",
        "2.  second layer is global average pooling layer which is used to prevent overfitting by reducing the number of parameters.\n",
        "\n",
        "1.   then a dense layer which has 16 hidden units and uses relu as activation function\n",
        "2.  the final layer is the output layer which uses sigmoid as activation function \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1WDSfAIPs31"
      },
      "source": [
        "model=keras.Sequential([keras.layers.Embedding(10000,16,input_length=500),\n",
        "                        keras.layers.GlobalAveragePooling1D(),\n",
        "                        keras.layers.Dense(16,activation='relu'),\n",
        "                        keras.layers.Dense(1,activation='sigmoid')])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pK_1v8v3hBZ"
      },
      "source": [
        "#compiling the model\n",
        "\n",
        "\n",
        "1.   Adam is used as optimization function for our model.\n",
        "2.   Binary cross entropy loss function is used as loss function for the model.\n",
        "\n",
        "1.   Accuracy is used as the metric for evaluating the model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPsQuds5QYud"
      },
      "source": [
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wJsxdk24H_g"
      },
      "source": [
        "In the next step we are going to train the model on our downloaded IMDB dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxUCLcRJQmBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa5be56b-84d3-4cfc-a363-dfba0f79220e"
      },
      "source": [
        "#training the model\n",
        "history=model.fit(train_data,train_labels,epochs=30,batch_size=512,validation_data=(test_data,test_labels))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "5/5 [==============================] - 1s 85ms/step - loss: 0.6933 - accuracy: 0.4853 - val_loss: 0.6932 - val_accuracy: 0.4935\n",
            "Epoch 2/30\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.6929 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.4793\n",
            "Epoch 3/30\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.6927 - accuracy: 0.5114 - val_loss: 0.6933 - val_accuracy: 0.4793\n",
            "Epoch 4/30\n",
            "5/5 [==============================] - 0s 38ms/step - loss: 0.6924 - accuracy: 0.5114 - val_loss: 0.6931 - val_accuracy: 0.4793\n",
            "Epoch 5/30\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.6921 - accuracy: 0.5114 - val_loss: 0.6930 - val_accuracy: 0.4793\n",
            "Epoch 6/30\n",
            "5/5 [==============================] - 0s 40ms/step - loss: 0.6917 - accuracy: 0.5114 - val_loss: 0.6928 - val_accuracy: 0.4793\n",
            "Epoch 7/30\n",
            "5/5 [==============================] - 0s 38ms/step - loss: 0.6914 - accuracy: 0.5114 - val_loss: 0.6926 - val_accuracy: 0.4793\n",
            "Epoch 8/30\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.6909 - accuracy: 0.5114 - val_loss: 0.6923 - val_accuracy: 0.4793\n",
            "Epoch 9/30\n",
            "5/5 [==============================] - 0s 38ms/step - loss: 0.6904 - accuracy: 0.5114 - val_loss: 0.6919 - val_accuracy: 0.4793\n",
            "Epoch 10/30\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.6898 - accuracy: 0.5114 - val_loss: 0.6914 - val_accuracy: 0.4793\n",
            "Epoch 11/30\n",
            "5/5 [==============================] - 0s 41ms/step - loss: 0.6891 - accuracy: 0.5114 - val_loss: 0.6908 - val_accuracy: 0.4793\n",
            "Epoch 12/30\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.6883 - accuracy: 0.5314 - val_loss: 0.6899 - val_accuracy: 0.5428\n",
            "Epoch 13/30\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.6872 - accuracy: 0.6072 - val_loss: 0.6889 - val_accuracy: 0.6044\n",
            "Epoch 14/30\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.6861 - accuracy: 0.6430 - val_loss: 0.6880 - val_accuracy: 0.6324\n",
            "Epoch 15/30\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.6848 - accuracy: 0.6736 - val_loss: 0.6869 - val_accuracy: 0.6393\n",
            "Epoch 16/30\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.6833 - accuracy: 0.6842 - val_loss: 0.6858 - val_accuracy: 0.6465\n",
            "Epoch 17/30\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.6817 - accuracy: 0.6911 - val_loss: 0.6844 - val_accuracy: 0.6549\n",
            "Epoch 18/30\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.6798 - accuracy: 0.7009 - val_loss: 0.6829 - val_accuracy: 0.6572\n",
            "Epoch 19/30\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.6778 - accuracy: 0.7115 - val_loss: 0.6815 - val_accuracy: 0.6779\n",
            "Epoch 20/30\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.6754 - accuracy: 0.7258 - val_loss: 0.6797 - val_accuracy: 0.6825\n",
            "Epoch 21/30\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.6728 - accuracy: 0.7323 - val_loss: 0.6773 - val_accuracy: 0.6855\n",
            "Epoch 22/30\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.6700 - accuracy: 0.7351 - val_loss: 0.6751 - val_accuracy: 0.6947\n",
            "Epoch 23/30\n",
            "5/5 [==============================] - 0s 39ms/step - loss: 0.6668 - accuracy: 0.7449 - val_loss: 0.6727 - val_accuracy: 0.7096\n",
            "Epoch 24/30\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.6634 - accuracy: 0.7624 - val_loss: 0.6704 - val_accuracy: 0.7314\n",
            "Epoch 25/30\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.6596 - accuracy: 0.7783 - val_loss: 0.6670 - val_accuracy: 0.7349\n",
            "Epoch 26/30\n",
            "5/5 [==============================] - 0s 38ms/step - loss: 0.6554 - accuracy: 0.7918 - val_loss: 0.6642 - val_accuracy: 0.7456\n",
            "Epoch 27/30\n",
            "5/5 [==============================] - 0s 40ms/step - loss: 0.6508 - accuracy: 0.8032 - val_loss: 0.6607 - val_accuracy: 0.7529\n",
            "Epoch 28/30\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.6460 - accuracy: 0.8093 - val_loss: 0.6566 - val_accuracy: 0.7544\n",
            "Epoch 29/30\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.6407 - accuracy: 0.8154 - val_loss: 0.6526 - val_accuracy: 0.7582\n",
            "Epoch 30/30\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.6352 - accuracy: 0.8244 - val_loss: 0.6488 - val_accuracy: 0.7663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FQWLj-i4dEK"
      },
      "source": [
        "Now we will be evaluating the loss and accuracy of our model on testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHLzEJI-RCWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8068a562-c6de-4994-86b9-9590b010e196"
      },
      "source": [
        "loss,accuracy=model.evaluate(test_data,test_labels)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "82/82 [==============================] - 0s 2ms/step - loss: 0.6488 - accuracy: 0.7663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhtQ2vt44tvd"
      },
      "source": [
        "As we can see our model is giving an accuracy of 88.58% on the testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWH8VZPmisqr"
      },
      "source": [
        "Now we are going to take a random review from our test dataset and check wether our model produces correct output or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiqpYZu7ipTa",
        "outputId": "e55e6d74-96b8-4fea-a22c-c97a4c96888e"
      },
      "source": [
        "index=np.random.randint(1,1000)\n",
        "user_review=test_reviews.loc[index]\n",
        "print(user_review)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reviews      <START i am a talent manager i have been for 1...\n",
            "Sentiment                                             positive\n",
            "Name: 469, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml5oNL-dj-Ix"
      },
      "source": [
        "As we can see the sentiment for the above review is positive, now we are going to take the integer format of this particular review which we already have in our preprocessed test data and then give it as an input to our model to check the prediction of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLJGmSkej9Jl",
        "outputId": "a99bc8d1-d016-43ad-eaf1-c7b3dfdf6e26"
      },
      "source": [
        "user_review=test_data[index]\n",
        "user_review=np.array([user_review])\n",
        "if (model.predict(user_review)>0.5).astype(\"int32\"):\n",
        "  print(\"positive sentiment\")\n",
        "else:\n",
        "  print(\"negative sentiment\")\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative sentiment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBfalQWOmg66"
      },
      "source": [
        "As we can see our model is now able to predict the sentiment of the review."
      ]
    }
  ]
}